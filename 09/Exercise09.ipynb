{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a827ed8c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2e3f125db91d98c2c3ace81981dfd3c",
     "grade": false,
     "grade_id": "cell-6a941dadcabee4ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Exercise Sheet No. 9\n",
    "\n",
    "---\n",
    "\n",
    "> Machine Learning for Natural Sciences, Summer 2022, Jun.-Prof. Pascal Friederich, pascal.friederich@kit.edu\n",
    "> \n",
    "> Deadline: 27.06.2022, 8 am\n",
    ">\n",
    "> Tutor: Payam Kalhor [payam.kalhor@kit.edu](mailto:payam.kalhor@kit.edu)  \n",
    "> **Please ask questions in the forum and only contact the tutor for issues regarding the grading**\n",
    "\n",
    "---\n",
    "\n",
    "**Topic**: This exercise sheet will focus on recurrent neural networks, in particular a GRU implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283655cd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "091ebf267cf1d8f8355c9413830633f0",
     "grade": false,
     "grade_id": "cell-d98bef211cb95e3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Problem and Dataset\n",
    "\n",
    "Recurrent neural networks are usually well suited for tasks where e.g. a time series of inputs need to be embedded into a fixed size latent representation and where the relationship in time within one sequence sample is important for predictions, e.g. to detect trends etc.\n",
    "\n",
    "For this exercise I selected a time series regression problem based on open data and with some local and present relevance:  \n",
    "As some of you might noticed Germany has a problem with air pollution in big cities like Berlin, Munich, Cologne or Stuttgart. To reduce the emissions of nitrogen dioxide general driving bans for diesel cars were discussed and introduced in some cities.  \n",
    "On the other hand air pollution is not only dependent on traffic but also on the weather: Wind brings fresh air and dilutes the city air, rain washes substances out of the air, etc.  \n",
    "Hence, a flexible driving ban which is only introduced for weekdays with weather conditions associated with a high pollution, e.g. no wind, longer dry periods, etc. could be introduced.  \n",
    "To allow planning for commuters the ban should be announced at least one day ahead.\n",
    "\n",
    "Hence, I collected weather data for the German meteorological office ([DWD](https://www.dwd.de/DE/klimaumwelt/cdc/cdc_node.html) 04928 Stuttgart Schnarrenberg) and NO2 data from the Federal Environmental Agency ([UBA](https://www.umweltbundesamt.de/daten/luft/luftdaten/stationen/eJzrXpScv9B4UXEykEhJXGVkYGSoa2Cma2i8qCRzkaHJorzUBYuKSxYsSUl0K4LLGpgB-SH5yKqTEycsyq1iW5Sb3LQ4J7HktIPnqnmvGuWOL87JSz_toHLOxeGTxWwADVorxg==) DEBW099 - Stuttgart Arnulf-Klett-Platz) for Stuttgart.\n",
    "\n",
    "All data is in 10 min. resolution. The weather data consists of:\n",
    "- Mean Temperature in 2m height `TT_10` ($°C$)\n",
    "- Relative Humidity `RF_10` ($\\%$)\n",
    "- Mean Wind speed `RF_10` ($\\frac{m}{s}$)\n",
    "- Mean Wind direction `DD_10` ($°$)\n",
    "- Sum of Sunshine Duration `SD_10` ($h$)\n",
    "- Precipitation height `RWS_10` ($mm$)\n",
    "\n",
    "The UBA data only has one column, the NO2 level in $\\frac{\\mu g}{m^3}$.\n",
    "\n",
    "We can now use the 6 weather features 7 one-hot encoded weekdays and the past measurements for NO2 to predict the maximum hourly mean NO2 level for the next 24 hour window.\n",
    "\n",
    "As the last exercises were quite intense, the Corona cases are low and the weather is nice, I think it is time for you to finally catch up with some social activities.   \n",
    "Hence, this exercise will only focus on the implementation of a gated recurrent unit as a Tensorflow layer and everything else is given.\n",
    "\n",
    "Nevertheless, I encourage you to explore the dataset and think about ways to improve the model. This task is not from any standard example dataset and there is quite some room for improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1016da7a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e14da31293b6a865eee8c5a8e959dbf8",
     "grade": false,
     "grade_id": "cell-90eaa0aed9d30c97",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eb571c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa4090941a7021e3843add52fbcc7f96",
     "grade": false,
     "grade_id": "cell-5d11117e7cf37ae6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_hdf('weather_no2_stuttgart.h5')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819d261",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b2ce604201c66d31e4284f9b0baaf2c",
     "grade": false,
     "grade_id": "cell-907a5f3343e003cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As recurrent NNs take long to train, we won't do a CV in this notebook, but only a simple train test split with a 20% test size.  \n",
    "\n",
    "Additionally, we need to have different shapes for the input to the GRU and to our benchmark model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636c5fd1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0960a1ec78575b9fb26a51b90711609f",
     "grade": false,
     "grade_id": "cell-197747169fb2df68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Weather data for 24 hours as input\n",
    "feature_len = 6*24\n",
    "\n",
    "# Predict 24 h ahead\n",
    "delay = 0\n",
    "\n",
    "# Extract samples starting every 6 hours\n",
    "step = 6*6\n",
    "\n",
    "# total number of extracted windows\n",
    "n_samples = int(np.floor((df.shape[0]-feature_len-delay-step)/step)) # samples are in groups\n",
    "print(\"number of samples\", n_samples) # 5839\n",
    "\n",
    "# scalers\n",
    "X_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "features = df\n",
    "features = pd.DataFrame(data=X_scaler.fit_transform(features), columns=features.columns)\n",
    "labels = df.UBA_DEBW099\n",
    "\n",
    "# One-hot encoded weekdays\n",
    "weekdays = LabelBinarizer().fit_transform(df.index.weekday)\n",
    "\n",
    "# One X for time series input where the second dimension is the length of the time series'\n",
    "X = np.zeros([n_samples, feature_len, features.shape[1]])\n",
    "\n",
    "# One X for the one-hot encoded weekdays, one for each sample (no time series here)\n",
    "X2 = np.zeros([n_samples, 7])\n",
    "\n",
    "# And one regression output\n",
    "y = np.zeros([n_samples, 1])\n",
    "\n",
    "for i in range(n_samples):\n",
    "    # Get the 24 hour feature data\n",
    "    X[i, :, :features.shape[1]] = features.iloc[i*step:i*step+feature_len, :]\n",
    "    \n",
    "    # Assign the weekday encoding\n",
    "    X2[i, :] = weekdays[i*step]\n",
    "\n",
    "    # max. hourly mean of the next 6 hrs:\n",
    "    h_max = 0\n",
    "    for n in range(int(step/6)):\n",
    "        current_max = labels.iloc[i*step+feature_len+delay+n*6:i*step+feature_len+delay+(n+1)*6, 0].mean()\n",
    "        #print(\"current max\" ,current_max)\n",
    "        h_max = max(h_max, current_max)\n",
    "    y[i, 0] = h_max\n",
    "\n",
    "    \n",
    "y = y_scaler.fit_transform(y)\n",
    "\n",
    "# Data for the benchmark\n",
    "X_ = np.concatenate([X.reshape(X.shape[0], X.shape[1]*X.shape[2]), X2], axis=1)\n",
    "X_train_, X_test_, y_train_, y_test_ = train_test_split(X_, y, test_size=.2, random_state=42)\n",
    "\n",
    "# Data for the GRU\n",
    "X_train, X_test, X2_train, X2_test, y_train, y_test = train_test_split(X, X2, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e418a69",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08c8e4d65c0dd1dcdda691d61c5eb44d",
     "grade": false,
     "grade_id": "cell-bb727a0e4c7137e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Benchmark\n",
    "\n",
    "When approaching a new problem with complex ML models, it is always important to test a trivial benchmark as a reference to evaluate whether the model complexity actually adds value. Here we can simply use a linear regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6373fb63",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5cd180237fb9c2d29ae3390277bb5c89",
     "grade": false,
     "grade_id": "cell-f8b662032e9899c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_regression():\n",
    "    return LinearRegression()\n",
    "\n",
    "\n",
    "def train(model, X_train, y_train, weights=None, validation_data=None, epochs=150, batch_size=512):\n",
    "    if model.__module__.split('.')[0] == 'keras':\n",
    "        history = model.fit(X_train,\n",
    "                            y_train,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            class_weight=weights,\n",
    "                            validation_data=validation_data\n",
    "                           )\n",
    "        return history\n",
    "    elif model.__module__.split('.')[0] == 'sklearn':\n",
    "        history = model.fit(X_train.reshape(X_train.shape[0], X_train.shape[1]), y_train)\n",
    "        return history\n",
    "    else:\n",
    "        raise Exception('Can only handle sklearn or keras models!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ae6d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9b628eaee6bad5a87d8d2a6335b7814",
     "grade": false,
     "grade_id": "cell-4c4aa31e22236803",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = linear_regression()\n",
    "train(model, X_train_, y_train_)\n",
    "y_pred_ = model.predict(X_test_)\n",
    "print(f'R2:  {r2_score(y_test_, y_pred_)}')\n",
    "print(f'MSE: {mean_squared_error(y_test_, y_pred_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3ddc61",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92a44988d778ca1e79fd887d4a566683",
     "grade": false,
     "grade_id": "cell-7dd8d39075f29d49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[10,10])\n",
    "ax.scatter(y_scaler.inverse_transform(y_test_), y_scaler.inverse_transform(y_pred_), alpha=0.5)\n",
    "ax.set_xlabel('True NO2')\n",
    "ax.set_ylabel('Predicted NO2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb1e305",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d8127b545d4b21a1cea2b5308db50e73",
     "grade": false,
     "grade_id": "cell-5a37d5e5df3f3413",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "So we get a trend but are far from a usable model. E.g. for a true NO2 value of 100 we get predictions ranging from 50 -120...\n",
    "\n",
    "Now to your actual task:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db8387",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "197c2936b5434ca63d462bc8a6566fad",
     "grade": false,
     "grade_id": "cell-479b7b303fda3bbd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Gated Recurrent Units (GRU)\n",
    "![gru](https://classic.d2l.ai/_images/gru_3.svg)\n",
    "\n",
    "The Gated Recurrent Unit can be viewed as a simplified yet the equally powerful version of the Long Short-Term Memory (LSTM) cell. Both of them have control on when a hidden state $H_t$ should be updated and also when it should be reset.\n",
    "\n",
    "In the GRU architecture, a sigmoid activation function is used in layers of gate $R_t$ and $Z_t$, therefore the output of these two gates are both vectors with entries in the interval $(0, 1)$. The gate is considered \"open\" when its output is close to $1$ and \"close\" if it outputs $0$.  \n",
    "By switching between \"open\" and \"close\", the reset gate $R_t$ has control on how much information from the previous states is stored in the candidate state $\\tilde{H_t}$. On the other hand $Z_t$ determines how much the new candidate state $\\tilde{H_t}$ is used.\n",
    "\n",
    "\n",
    "Let us describe how the activation of the $j$-th hidden unit is computed.\n",
    "\n",
    "## Reset gate $R_t$\n",
    "\n",
    "$r_{tj} = \\sigma([W_{r}x_t]_j + [U_{r}h_{t-1}]_j)$  \n",
    "\n",
    "\n",
    "## Update gate $Z_t$\n",
    "\n",
    "$z_{tj} = \\sigma([W_{z}x_t]_j + [U_{z}h_{t-1}]_j)$\n",
    "\n",
    "\n",
    "## Candidate state $\\tilde{H_j}$\n",
    "\n",
    "$\\tilde{h_j} = tanh([W_{h}x_t]_j + [U_{h}(r_{tj}\\otimes h_{t-1})]_j)$\n",
    "\n",
    "\n",
    "## Output $h_{t}$\n",
    "\n",
    "$h_{t} = z_{tj}\\otimes h_{t-1} + (1 - z_{tj})\\otimes \\tilde{h_{j}}$\n",
    "\n",
    "\n",
    "\n",
    "1. $W_{r}$, $W_{z}$, $W_{h}$ are the weight matrices of each of the three layers for their connection to the input vector $x_t$ at the current timestep $t$.\n",
    "2. $U_{r}$, $U_{z}$, $U_{h}$ are the weight matrices of each of the three layers for their connection to the hidden state $h_{t-1}$ from the previous timestep $t$.\n",
    "3. $\\sigma$ stands for sigmoid activation function, and $tanh$ is the tangent function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6bb652",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e0e2f9c206ee3e9ad6b2fc655eb5014",
     "grade": false,
     "grade_id": "cell-fd9762301fb16bde",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class SimpleGRUcell(keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(SimpleGRUcell, self).__init__(**kwargs)\n",
    "        # output dimension\n",
    "        self.units = units\n",
    "        # state dimension\n",
    "        self.state_size = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "                \n",
    "        # Initializer for the input weights matrix, used for the linear transformation of the inputs\n",
    "        w_init = initializers.GlorotUniform()\n",
    "        # Initializer for the state weights matrix, used for the linear transformation of the states\n",
    "        u_init = initializers.Orthogonal()\n",
    "        \n",
    "        # weights of gate r\n",
    "        # input weights\n",
    "        self.w_r = None\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        # state weights\n",
    "        self.u_r = None\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # weights of gate z\n",
    "        # input weights\n",
    "        self.w_z = None\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        # state weights\n",
    "        self.u_z = None\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # weight of h_candidate\n",
    "        # input weights:\n",
    "        self.w_h = None\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        # state weights\n",
    "        self.u_h = None\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        self.build = True\n",
    "        \n",
    "    def call(self, x_t, states):\n",
    "        h_tm1 = states[0]\n",
    "        \n",
    "        r_t = None\n",
    "        z_t = None\n",
    "        h_candidate = None\n",
    "        h_t = None\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return h_t, [h_t]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0982a460",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "625491f7ee35e5be2dd1af0294db6cab",
     "grade": false,
     "grade_id": "cell-7c9789ea402eebfb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next we implement a simple RNN. We need to differentiate between the time series input and the weekday input to the network and hence use the functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fee43db",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6517426ec0fabd3d2f6400b3e4b70e4d",
     "grade": false,
     "grade_id": "cell-86a1616ff543d691",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# R2 metric using tensors\n",
    "def r2(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred )) \n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "\n",
    "def rnn(X, y):\n",
    "    cell = SimpleGRUcell(15)\n",
    "    \n",
    "    # First input for time series\n",
    "    in_0 = keras.layers.Input(shape=X.shape[1:])\n",
    "        \n",
    "    # Second input for weekdays\n",
    "    in_1 = keras.layers.Input(shape=(7,))\n",
    "    \n",
    "    # RNN layer\n",
    "    x_0 = keras.layers.RNN(cell)(in_0)\n",
    "    # A little bit of regularization\n",
    "    x_1_d = keras.layers.Dropout(.2)(x_0)\n",
    "    # Concattenating the GRU embedding of the input sequence and the weekday encoding\n",
    "    x_1 = keras.layers.Concatenate()([in_1, x_1_d])\n",
    "    x_2 = keras.layers.Dense(units=1, activation='linear')(x_1)\n",
    "    \n",
    "    # Here we pass the input layers as a list\n",
    "    model = keras.Model(inputs=[in_0, in_1], outputs=x_2)\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    loss = keras.losses.MeanSquaredError()\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss,\n",
    "                  metrics=[r2]\n",
    "                 )\n",
    "    return model\n",
    "\n",
    "\n",
    "gru_model = rnn(X_train, y_train)\n",
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab452c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train(gru_model, [X_train, X2_train], y_train, validation_data=([X_test, X2_test], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e55d7d7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "581e18177e821df0018bef999b42d14f",
     "grade": false,
     "grade_id": "cell-1edd1609c7b1ffb9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[9,7])\n",
    "ax.plot(history.history['r2'], 'b', label='train')\n",
    "ax.plot(history.history['val_r2'], 'r', label='val')\n",
    "ax.set_title('Training Curve')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('R2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b78ab35",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f4e48cc4989c349159d470e9c3e3f5bd",
     "grade": false,
     "grade_id": "cell-38a920ff85c2b824",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = gru_model.predict([X_test, X2_test])\n",
    "print(f'R2:  {r2_score(y_test, y_pred)}')\n",
    "print(f'MSE: {mean_squared_error(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d72590",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5fcc6623ba30783d43c855ded5ab9fc",
     "grade": false,
     "grade_id": "cell-683d34287be75c2c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[10,10])\n",
    "ax.scatter(y_scaler.inverse_transform(y_test), y_scaler.inverse_transform(y_pred), alpha=0.5)\n",
    "ax.set_xlabel('True NO2')\n",
    "ax.set_ylabel('Predicted NO2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43caa10d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c805df6d6ef210b4b4bf9b2f102e2e3",
     "grade": true,
     "grade_id": "GRU-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This determines whether you passed\n",
    "r2_gru = r2_score(y_test, y_pred)\n",
    "assert r2_gru > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aa3c84",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f83bca88f51fa4abf3d91d58705b1cd0",
     "grade": false,
     "grade_id": "cell-1da57b07c76de195",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You should be able to reach an R2 of around 0.65 here.  \n",
    "How do you rate this result? Also take into consideration the number of trainable parameters in the two models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75552d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a77641d12afffbae31de2c784969c7e",
     "grade": false,
     "grade_id": "cell-f55013e9d827e437",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now go and enjoy the sun!\n",
    "\n",
    "...and while you are doing that, think about how to improve the model, by architecture, feature engineering, etc. I hope for a little discussion when we go through the exercise together after the deadline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('aimat')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f01025c16334be4b338395331e8b88f79df1ef1ca39934b9fbad79cfa8c0ed46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
