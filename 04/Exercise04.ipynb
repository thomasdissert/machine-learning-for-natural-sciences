{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b88e5098fa6356c9ca2504364d714a9",
     "grade": false,
     "grade_id": "cell-23f9a13fcbe0410a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise Sheet No. 4\n",
    "\n",
    "---\n",
    "\n",
    "> Machine Learning for Natural Sciences, Summer 2022, Jun.-Prof. Pascal Friederich, pascal.friederich@kit.edu\n",
    "> \n",
    "> Deadline: 16.05.2022, 8am\n",
    ">\n",
    "> Tutor: Chen Zhou chen.zhou@kit.edu\n",
    ">\n",
    "> Please ask questions in the forum and only contact the tutor for issues regarding the grading\n",
    "\n",
    "---\n",
    "\n",
    "**Topic**: \n",
    "This exercise sheet will focus on the math basics for ML. You will implement another simple ML algorithm, **linear regression** (LR), instead of decision tree to work on the same task you have already seen in exercise 02. Gradient decent, mean square error loss function, and mean absolute error function are covered in this exercise. And you will learn to use ridge regression to control overfitting.\n",
    "\n",
    "We will use [seaborn](https://seaborn.pydata.org/index.html) as a more abstract plotting interface in this exercise. You can install it into your current aimat conda environment by executing the following command lines:\n",
    "\n",
    "`conda activate aimat`\n",
    "\n",
    "`conda install seaborn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "19cf46e1f5bdbd5e2976ccb33476ac69",
     "grade": false,
     "grade_id": "cell-6754d0c5a478283c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Linear Regression\n",
    "In assignment 2, we have seen the use of decision tree for iris species classification. Now, let's try to solve the same problem with another simple machine learning technique: linear regression (LR).   \n",
    "\n",
    "Linear regression uses a linear combination of features to predict a target. In our case we can use a linear combination of the four flower descriptors to predict the species.\n",
    "\n",
    "In this assignment, you will learn to implement the Loss function and Gradient Decent for optimization. Then, we will see that even small model like LR can overfit, and how regularization (ridge regression) can help against this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import random\n",
    "from functools import wraps\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c85a1f4f71c05ff0d236a0fed190d64f",
     "grade": false,
     "grade_id": "cell-1cb644d0633a97e5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Preprocessing\n",
    "Let's start with preprocessing the dataset, as already learned from assignment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_name(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        result = func(*args, **kwargs)\n",
    "        print(f\"\\n{func.__name__}:\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "def log_shape(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        result = func(*args, **kwargs)\n",
    "        print(f\"\\tshape: {result.shape}\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "def log_columns(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        result = func(*args, **kwargs)\n",
    "        print(f\"\\tcolumns: {result.columns.values}\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@log_columns\n",
    "@log_shape\n",
    "@log_name\n",
    "def load(df, path):\n",
    "    \"\"\"Loads the dataset from path.\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "\n",
    "@log_columns\n",
    "@log_shape\n",
    "@log_name\n",
    "def convert_to_categorical(df, col_name: str):\n",
    "    df[col_name] = df[col_name].astype('category')\n",
    "    return df\n",
    "\n",
    "@log_columns\n",
    "@log_shape\n",
    "@log_name\n",
    "def add_class_labels(df):\n",
    "    df['class'] = df['species'].cat.codes\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df = (\n",
    "    df.pipe(load, 'iris.csv')\n",
    "      .pipe(convert_to_categorical, 'species')\n",
    "      .pipe(add_class_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "85effffc96fa8ee96fc88e32ca076c6c",
     "grade": false,
     "grade_id": "cell-0b250c3f733fc1f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Plot the data\n",
    "It is always a good practice to have some inspection on the dataset. This provides us with information about the data such as its structure, range, outliers etc., which may help on the design of ML algorithm. There are numerous ways to visualize data, and introduced here is the one called \"Pairplot\", which displays pairwise relationships in a dataset. Pairplot can be implemented easily with the [seaborn](https://seaborn.pydata.org/generated/seaborn.pairplot.html?highlight=pairplot#seaborn.pairplot) library. Here we used it to show the pairwise relationships among iris features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df.loc[:,:'species'], hue= 'species')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e843b7e6fe84c67a0bf4215fba3843a",
     "grade": false,
     "grade_id": "cell-d45fb09151e8b752",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question**: out of these combinations of features, which pair has the strongest linear correlation relationship?\n",
    "\n",
    "**1.** sepal_length and petal_width    \n",
    "**2.** sepal_width and petal_length    \n",
    "**3.** petal_length and petal_width    \n",
    "**4.** sepal_width and petal_length    \n",
    "**5.** sepal_length and sepal_width\n",
    "\n",
    "(To learn more about the linear correlation, please refer to [here](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient))\n",
    "\n",
    "Assign the number of your choice to the variable `A`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b38b5ddd333c82caa34fbc6c79ffe8f5",
     "grade": false,
     "grade_id": "Correlation_Plot-answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "A = (\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01b5a2704e7d7d38d9b996947e8536a0",
     "grade": true,
     "grade_id": "Correlation_Plot-test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(A) == int # Please make sure your input is a number.\n",
    "\n",
    "# Hidden test below\n",
    "# 1 point: select the correct choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf508cff095bf97f30df88ea7d77e287",
     "grade": false,
     "grade_id": "cell-ebcde626c67fdc9b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Generate training/validation set\n",
    "Now, for our machine learning task, let's extract feature matrix `X` and label matrix `Y` from the dataframe `df`. Please implement the `split_X_Y` function that returns feature `X` as a $150 \\times 4$ numpy array, as well as label `Y` as a $150 \\times 1$ numpy array. `X` should contain values from column \"sepal_length\", \"sepal_width\", \"petal_length\" and \"petal_width\". `Y` should be the value of column \"class\". You may use methods such as `.reshape()` etc., and attributes like `.values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "367e827527df3e51675bc0e987830169",
     "grade": false,
     "grade_id": "Train_Test_Split-answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def split_X_Y(df):\n",
    "    \"\"\"split the dataframe into feature matrix X and label matrix Y\"\"\"\n",
    "    X = None # please update this in your solution\n",
    "    Y = None # please update this in your solution\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3cc2bd95935b0c7159cabe93c65efe50",
     "grade": true,
     "grade_id": "Train_Test_Split-test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# feature/target split\n",
    "X, Y = split_X_Y(df)\n",
    "assert X.shape == (150, 4)\n",
    "assert Y.shape == (150, 1)\n",
    "\n",
    "# Hidden test below\n",
    "# 2 points: values in X and Y are checked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d2eee2a79cc4aef12cacfaba07e910d",
     "grade": false,
     "grade_id": "cell-af56efe9d9027cdf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We have learned that the linear model tries to fit the function:\n",
    "\\begin{align}\n",
    "y = \\omega^T x + \\omega_0 = \\sum^n_{i=1} \\omega_i x_i + \\omega_0\n",
    "\\end{align}\n",
    "Where $\\omega$ is the weight matrix and $\\omega_0$ is the bias term. Let $x_0 = 1$, this function can be rewritten into:\n",
    "\\begin{align}\n",
    "y =  \\sum^n_{i=1} \\omega_i x_i + \\omega_0 x_0 = \\sum^n_{i=0} \\omega_i x_i = X \\Omega^T\n",
    "\\end{align}\n",
    "$\\Omega$ can be initialized randomly, and optimized later through training. In our case $\\Omega$ has five elements - the first one for the bias and rest four for the weights.\n",
    "\n",
    "**Please note that $\\Omega$ is written as a row vector in many text books for display convenience and is actually a column vector during implementation. That's why we have $\\Omega^T$ in the equation to denote the convertion. Here we use `numpy.T` to make our code and equation consistent.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random initialization for weights and bias\n",
    "np.random.seed(0)\n",
    "omega = np.random.randn(1,5).T\n",
    "omega"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d5177e914af51aedb7102d87477dfac3",
     "grade": false,
     "grade_id": "cell-75bfd7154053f864",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's contract $x_0 = 1$ to our feature matrix `X` to obtain the input matrix for LR model. Please do that by stacking a new column to `X` as the first colunm with all values equal to $1$. This may be easily implemented with the numpy method `.hstack()` . You may find more details [here](https://numpy.org/doc/stable/reference/generated/numpy.hstack.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ae23bb75a457a6663e666344ca47644",
     "grade": false,
     "grade_id": "Add_bias_term_to_X-answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Update feature matrix X with x0 = 1\n",
    "X = (\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e35000ff530f923a8cf00269c451d204",
     "grade": true,
     "grade_id": "Add_bias_term_to_X-test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert X.shape == (150, 5) # now the shape of X should change from (150, 4) to (150, 5)\n",
    "\n",
    "# 1 point: values in X are checked\n",
    "assert (X[:, 0] == 1).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce376689dff772ce6037f0b200f48210",
     "grade": false,
     "grade_id": "cell-3063ec8d8014755b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In case you are interested, there's a \"closed-form solution\" that estimates the best parameter set $\\Omega^*$ by solving the equation:\n",
    "\\begin{align}\n",
    "\\Omega^* = (X^TX)^{-1}X^Ty\n",
    "\\end{align}\n",
    "Watch this [video](https://www.coursera.org/lecture/ml-regression/approach-1-closed-form-solution-G9oBu) from coursera for detailed explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5de70719b7511a3ceeff8793f6dcebbe",
     "grade": false,
     "grade_id": "cell-3b6704681c9de592",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Gradient Decent\n",
    "For more complex tasks, a closed form solution often doesn't exist and hence we will introduce another approach here, aka Gradient Decent (GD) algorithm, to solve the linear regression. Gradient Decent works by updating the weight vector incrementally after each epoch. Read more [here](https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fcfcf04a2c8555ce01e67174644f6b2a",
     "grade": false,
     "grade_id": "cell-2aac0b9bb5d8eb89",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Firstly, let's do a train-valid split for our dataset, as we have already seen in Assignment 2. The `x_train` and `y_train` will be our training set, and `x_val`, `y_val` will be our validation set. Just to refresh your memory, training set is used for training to improve our model performance, while validation set tests the model with unseen data. Testing our model with validation set is crucial especially when we want to know the generalization ability of the model. You will see more details later in the Overfitting & Ridge regression section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50b9863a6dc10fa6275c9f973cfbc578",
     "grade": false,
     "grade_id": "cell-08de0d05761681ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here we randomly shuffle the dataset and take $120$ $(80\\%)$ data points as training set. The rest $30$ $(20\\%)$ are used as validation set. There's also a useful method `sklearn.model_selection.train_test_split()` from scikit-learn for the same purpose. You can find its documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = list(range(0, 150))\n",
    "random.shuffle(idx)\n",
    "x_train = X[idx[:120]]\n",
    "y_train = Y[idx[:120]]\n",
    "x_val = X[idx[120:]]\n",
    "y_val = Y[idx[120:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88c50b633f345ea1fe2f6ec176100ac3",
     "grade": false,
     "grade_id": "cell-a0ed23a25e5d3cd1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Cost function\n",
    "The mean squared error (mse) is a common loss function that measures the average squared difference between predict and real values. Here, it is defined as:\n",
    "\\begin{align}\n",
    "MSE = \\frac{1}{2n}\\sum^n_{i=1}(\\text{y_pred}_i - \\text{y_real}_i)^2\n",
    "\\end{align}\n",
    "The $\\frac{1}{2}$ in equation is just for convenience when computing the gradient (see next step). Recall that $y = X \\Omega^T$, so we have:\n",
    "\\begin{align}\n",
    "MSE = \\frac{1}{2n}\\sum^n_{i=1}(x_i \\Omega^T - \\text{y_real}_i)^2\n",
    "\\end{align}\n",
    "Please implement the `mean_square_error()` that returns MSE. Methods/attribute you may need are `numpy.sum()` and `numpy.dot()`. Although we use a $\\sum$ in the notation you can use your knowledge from exercise 3 to compute the following using only numpy vectors and matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e486788d4f7733d255556ab1f2c21279",
     "grade": false,
     "grade_id": "MSE-answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def mean_square_error(x, y, omega):\n",
    "    \"\"\"\n",
    "    return the mean suqared error.\n",
    "    \n",
    "    Args:\n",
    "        x (numpy.ndarray): numpy array of features.\n",
    "        y (numpy.ndarray): numpy array of corresponding classes.\n",
    "        omega (numpy.ndarray): weight vector.\n",
    "        \n",
    "    Returns:\n",
    "        mse (numpy.float64): mean squared error.\n",
    "    \"\"\"\n",
    "    mse = None # please update this in your solution\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ac34305df2c1a12ea6daea91844a383",
     "grade": true,
     "grade_id": "MSE-test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 1 point: a simple test to check if your mse function works\n",
    "a = np.array([[1.0, 2.0], [2.0, 2.0]])\n",
    "b = np.array([[1.0], [2.0]])\n",
    "w = np.array([[1.0, 1.0]]).T\n",
    "result = mean_square_error(a, b, w)\n",
    "assert result - 2.0 <= 0.01\n",
    "assert str(type(result)) == \"<class 'numpy.float64'>\"\n",
    "\n",
    "# Hidden test below\n",
    "# 1 point: further tests to check the output of the mse function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e31a05f980305a18350dba0e9f8ced1",
     "grade": false,
     "grade_id": "cell-0c61b6eda0caba8e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "After each epoch $t$, the Gradient Decent algorithm updates the weight vector in the direction of the negative gradient in order to reduce the cost function. The gradient is simply the partial derivative of mean squared error to the weight.\n",
    "\n",
    "You can deduce the derivative yourself for practice. Just run the following cell to render the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5519dbce553f79b87f0f48d0d119c1b",
     "grade": false,
     "grade_id": "cell-4107228f483c16f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(\"\\\\begin{align}\"\n",
    "                 \"\\\\frac{\\\\partial MSE}{\\\\partial \\\\Omega} &= \"\n",
    "                 \"\\\\frac{1}{n} \\\\sum^n_{i=0}(x_i \\\\Omega^T - \\\\text{y_real}_i) x_i \\\\\\\\\"\n",
    "                 \"\\\\Omega(t+1) &= \\\\Omega(t) - \\\\alpha \\\\frac{\\\\partial MSE}{\\\\partial \\\\Omega(t)}\"\n",
    "                 \"= \\\\Omega(t) - \\\\frac{\\\\alpha}{n} \\\\sum^n_{i=0}(x_i \\\\Omega^T - \\\\text{y_real}_i) x_i\"\n",
    "                 \"\\\\end{align}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c24bac7edb1fd3c5ab47c18b3f22bdb3",
     "grade": false,
     "grade_id": "cell-3b243d3fd3dba44a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The $\\alpha$ is the learning rate that controls the step size for the update after each iteration.\n",
    "\n",
    "Please implement the `weight_update_function()` that returns updated $\\Omega$. Methods/attribute you may need are `numpy.dot()` and `numpy.reshape()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a083c02d7b2dc3246f7e16d4d654408d",
     "grade": false,
     "grade_id": "Gradient_Descent-answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def weight_update_function(x, y, omega, alpha):\n",
    "    \"\"\"\n",
    "    return updated set of weights\n",
    "    \n",
    "    Args:\n",
    "        x (numpy.ndarray): numpy array of features.\n",
    "        y (numpy.ndarray): numpy array of corresponding classes.\n",
    "        omega (numpy.ndarray): weight vector.\n",
    "        alpha (float): learning rate.\n",
    "        \n",
    "    Returns:\n",
    "        omega_updated (numpy.ndarray): the updated weight vector.\n",
    "    \"\"\"\n",
    "    omega_updated = None # please update this in your solution\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return omega_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "913c1c99801b0221c7637a1d5d4295f6",
     "grade": true,
     "grade_id": "Gradient_Descent-test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 1 points: check the output value and shape of your weight_update_function\n",
    "result = weight_update_function(a, b, w, 0.001)\n",
    "assert abs(np.mean(result) - 0.996) <= 0.01\n",
    "assert result.shape == w.shape\n",
    "\n",
    "# Hidden test below\n",
    "# 1 point: further tests to check the output of the weight_update_function function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88fb4084875e771a0f50675db6e2b1dd",
     "grade": false,
     "grade_id": "cell-7425ef1050a146d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Training & plot\n",
    "To monitor the training process, here we use mean absolute error (mae):\n",
    "\\begin{align}\n",
    "MAE = \\frac{1}{n}\\sum^n_{i=1} |\\text{y_pred}_i - \\text{y_real}_i|\n",
    "\\end{align}\n",
    "Please implement the `mean_absolute_error()` that returns MAE. Methods/attribute you may need are `numpy.sum()`, `numpy.dot()` and `numpy.abs()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9d8b2d6559688701f020c43a00b5773",
     "grade": false,
     "grade_id": "MAE-answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def mean_absolute_error(x, y, omega):\n",
    "    \"\"\"\n",
    "    return the mean absolute error\n",
    "    \n",
    "    Args:\n",
    "        x (numpy.ndarray): numpy array of features.\n",
    "        y (numpy.ndarray): numpy array of corresponding classes.\n",
    "        omega (numpy.ndarray): weight vector.\n",
    "        \n",
    "    Returns:\n",
    "        mse (numpy.float64): mean absolute error.\n",
    "    \"\"\"\n",
    "    mae = None # please update this in your solution\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3907997aadafca1c64f439e9d88b0b0b",
     "grade": true,
     "grade_id": "MAE-test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 1 points: check the output value and shape of your mean_absolute_error function\n",
    "result = mean_absolute_error(a, b, w)\n",
    "assert np.round(result)  == 2.0\n",
    "assert str(type(result)) == \"<class 'numpy.float64'>\"\n",
    "\n",
    "# Hidden test below\n",
    "# 1 point: further tests to check the output of the mean_absolute_error function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0cc44d69ab7bd4ebe63f3a7d333609ee",
     "grade": false,
     "grade_id": "cell-851877d35d2dd051",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For the last step before training the model, let's set up the learning rate and number of iterations. We use `J_train` and `J_val` to record mean absolute errors for each epoch of the training and validation processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15000 # number of updates to the weight\n",
    "alpha = 0.001 # learning rate\n",
    "J_train = np.zeros(epochs) # MAE of training process\n",
    "J_val = np.zeros(epochs) # MAE of validation process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f24b444c43462e994bda314069de1b0d",
     "grade": false,
     "grade_id": "cell-13d06de7e4e3c1f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, let's train our model with the `weight_update_function()` and record mean absolute errors through `mean_absolute_error()` for the training/validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fec118b0de70dbb8ddf9e04f7939bdd7",
     "grade": false,
     "grade_id": "Train_Loop-answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "omega = np.random.randn(1,5).T # initialize weight/bias matrix randomly\n",
    "\n",
    "# begin training\n",
    "for i in range(epochs):\n",
    "    \"\"\"\n",
    "    for each epoch, record the mae for training and validation process\n",
    "    use the weight_update_function to update the omega\n",
    "    \"\"\"\n",
    "    J_val[i] = None # please update this in your solution\n",
    "    J_train[i] = None # please update this in your solution\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d7d69b15751f46e4b78ad68acb075fcf",
     "grade": true,
     "grade_id": "Train_Loop-test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 2 point: check the results of the training process \n",
    "assert J_train[-1] > 0\n",
    "assert np.std(J_val[-10:]) <= 1e-3 # the training process should converge\n",
    "\n",
    "# Hidden tests below\n",
    "# 1 point: further tests for the training/validation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9355fea4e3327f217dcc560b627f4b45",
     "grade": false,
     "grade_id": "cell-ae74de811a5efbf3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here the `plot_training_curve()` is implemented to visualize the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curve(MAE_train, MAE_val, epochs):\n",
    "    \"\"\"Plot the mean absolute error for training/validation process\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 5))\n",
    "    ax.plot(np.arange(epochs), MAE_train, label='Training')\n",
    "    ax.plot(np.arange(epochs), MAE_val, label='Validation')\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_ylabel(\"Mean Absolute Error\")\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_title(\"Mean Absolute Error vs Epochs\")\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_curve(J_train, J_val, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "33da3798480cdc8156fe22aa0d2bfdc6",
     "grade": false,
     "grade_id": "cell-8f5ede23cbae5e06",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question:** You may notice that we have implemented the `mean_square_error()` function but seemed to never use it in the training process. Please think about the reason and write it down here.\n",
    "Hint: double check the deduce process of the Gradient Decent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29a0dfd2406aecf7c479b035fadf956e",
     "grade": false,
     "grade_id": "mse_question_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "your_answer = (\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfec26ba20e0beb13896c2b51c9759cd",
     "grade": true,
     "grade_id": "mse_question_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 1 point for the answer\n",
    "assert type(your_answer) == str\n",
    "assert len(your_answer) > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8743b24d2e9f9e65372b94b56895060d",
     "grade": false,
     "grade_id": "cell-d1d5ee5a2c54fcc1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Accuracy\n",
    "Now, with the optimized weight vector, let's implement `prediction()` to get the predicted class labels from our linear model. You may need `numpy.dot()` for the computation, and `numpy.round()` to round up the results into integers (because our class labels are integer 0, 1, 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27a4a23f6e8442d1845b40067bc539f3",
     "grade": false,
     "grade_id": "cell-3dcb65242819c457",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def prediction(x, omega):\n",
    "    \"\"\"\n",
    "    Return predicted labels.\n",
    "    \n",
    "    Args:\n",
    "        x (numpy.ndarray): the feature matrix.\n",
    "        omega (numpy.ndarray): optimized weight vector\n",
    "        \n",
    "    Returns:\n",
    "        y_pred (numpy.ndarray): predicted labels.\n",
    "    \"\"\"\n",
    "    y_pred = None # please update this in your solution\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0155c08ec7f868957a4d891bae6c1233",
     "grade": true,
     "grade_id": "Accuracy-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 1 point\n",
    "Y_pred = prediction(X, omega)\n",
    "assert Y_pred.shape == Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23cadf0bc3dbc578e618cd3b67711cc8",
     "grade": false,
     "grade_id": "cell-3ff8c071f6a86c8b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "With `Y_pred`. Please implement `accuracy()` that calculates the prediction accuracy.\n",
    "\n",
    "Hint: prediction accuracy is computed by counting the number of predictions that match the real values, and then devide by the number of total instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b300d319ef864e19997142e58477cc4",
     "grade": false,
     "grade_id": "cell-1ecb0fa5853735c9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of prediction.\n",
    "    \n",
    "    Args:\n",
    "        y_pred (numpy.ndarray): the predicted class labels.\n",
    "        y (numpy.ndarray): the real class labels.\n",
    "        \n",
    "    Returns:\n",
    "        acc (numpy.float64): calculated accuracy in float.\n",
    "    \"\"\"\n",
    "    acc = None # please update this in your solution\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy(Y_pred, Y)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a50db8885860f7661b34495f10d8cca5",
     "grade": true,
     "grade_id": "accuracy-test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert str(type(acc)) == \"<class 'numpy.float64'>\"\n",
    "\n",
    "# Hidden test below\n",
    "# 1 point: test the output value of the accuracy() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e2b70debd6883360e007b365656c4e64",
     "grade": false,
     "grade_id": "cell-cfd646f9a9655148",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Visualize predictions vs ground truth\n",
    "For the last step, let's visualize our predictions v.s. the real labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predict_vs_real(Y_pred, Y):\n",
    "    \"\"\"Plot the predictions vs ground truch\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 5))\n",
    "    ax.scatter(np.arange(1, 151, 1), Y_pred, label='Predictions')\n",
    "    ax.plot(np.arange(1, 151, 1), Y, label='Ground Truch', color='red')\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    plt.yticks(np.arange(0, 3, 1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predict_vs_real(Y_pred, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "77a6d3a61915f65eb5c6d77f51c827ba",
     "grade": false,
     "grade_id": "cell-165d84e42ab77f0d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Please feel free to play around with the model and training process. You may adjust the learning rate and number of epochs to see how the trainig curve changes (do not forget to re-initialize the weight vector). You may also try different train-validation split ratio to see the effect.\n",
    "\n",
    "You may notice that the training results differ a lot with different hyperparameters (learning rate, omega initialization, etc.). This is one limitation of such simple ML model. Later in this semester, you will see and implement more sophisticated ML algorithms that can make more stable and accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28746d6c2e44b1f4af8c15eb64e4157c",
     "grade": false,
     "grade_id": "cell-926be4a613646f05",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Overfitting & Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8cd31d89c809cdb62e872fdbe6d3f75",
     "grade": false,
     "grade_id": "cell-59d2a351bd90a39f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "When finishing this assignment, you may (and may not, depending on the training set setup) sometimes observe the gap between the training curve and validation curve (validation error higher than training error) that cannot be diminished by increasing the number of epochs. This phenomenon is known as \"overfitting\". It happens when the model gets too complex so that it fits the training set perfectly, but loses the generalization ability towards unseen data from validation/test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3f85034fb8afc6192217ba975c6bbf64",
     "grade": false,
     "grade_id": "cell-0f409bfd47dc63d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ridge regression, also known as L2 regularization, is a useful technique to restrict our model from getting too complicated and reduce the effect of overfitting. It works by simply adding a penalty term to the cost function. In our case, the penalty term is $||\\Omega||^2$. It prefers lower absolute values of weights thus reduce the model complexity. For more information, please refer to [here](https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed66f8a45d23d9e0cb18650617766cf7",
     "grade": false,
     "grade_id": "cell-a2fef35e4cc07408",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "By introducting the penalty term, our cost function becomes:\n",
    "\\begin{align}\n",
    "MSE = \\frac{1}{2}(\\frac{1}{n}\\sum^n_{i=1}(\\text{y_pred}_i - \\text{y_real}_i)^2 + \\lambda ||\\Omega||^2)\n",
    "\\end{align}\n",
    "$\\lambda$ is the coefficient to adjust the regularization effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5efa06c5dcae30f8348db9973492ee0e",
     "grade": false,
     "grade_id": "cell-6e8209261b08ce72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "While the mean absolute error calculation stays the same, the weight update function becomes:\n",
    "\\begin{align}\n",
    "\\Omega = \\Omega - \\alpha \\frac{\\partial MSE}{\\partial \\Omega} = \\Omega - \\alpha(\\frac{1}{n} \\sum^n_{i=0}(x_i \\Omega^T - \\text{y_real}_i) x_i + \\lambda ||\\Omega||) \n",
    "\\end{align}\n",
    "Now, please implement the new cost function `mean_square_error_ridge()` and `weight_update_function_ridge()` with the penalty term. You may use `mean_square_error()` and `weight_update_function()` that are implemented earlier as references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "101ae3202bf436d3b6d27d4bb9767982",
     "grade": false,
     "grade_id": "cell-54d887bbc3b81ff8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def mean_square_error_ridge(x, y, omega, lam):\n",
    "    \"\"\"\n",
    "    return the mean suqared error.\n",
    "    \n",
    "    Args:\n",
    "        x (numpy.ndarray): numpy array of features.\n",
    "        y (numpy.ndarray): numpy array of corresponding classes.\n",
    "        omega (numpy.ndarray): weight vector.\n",
    "        lam (float): ridge regression coefficient.\n",
    "        \n",
    "    Returns:\n",
    "        mse (numpy.float64): mean squared error.\n",
    "    \"\"\"\n",
    "    mse = None # please update this in your solution\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7ddfe38da98af8554207b635b3cde4b",
     "grade": true,
     "grade_id": "Ridge_Regression-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 1 point: check the output value\n",
    "a = np.array([[1.0, 2.0], [2.0, 2.0]])\n",
    "b = np.array([[1.0], [2.0]])\n",
    "w = np.array([[1.0, 1.0]]).T\n",
    "assert np.round(mean_square_error_ridge(a, b, w, 0.1), 1) == 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "784889a4ede4716067acc74615541581",
     "grade": false,
     "grade_id": "cell-a97bb7ee36704ee5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def weight_update_function_ridge(x, y, omega, alpha, lam):\n",
    "    \"\"\"\n",
    "    return updated set of weights\n",
    "    \n",
    "    Args:\n",
    "        x (numpy.ndarray): numpy array of features.\n",
    "        y (numpy.ndarray): numpy array of corresponding classes.\n",
    "        omega (numpy.ndarray): weight vector.\n",
    "        alpha (float): learning rate.\n",
    "        lam (float): ridge regression coefficient.\n",
    "        \n",
    "    Returns:\n",
    "        omega_updated (numpy.ndarray): the updated weight vector.\n",
    "    \"\"\"\n",
    "    omega_updated = None # please update this in your solution\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return omega_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92ca037392d333332f1706f8caa732a8",
     "grade": true,
     "grade_id": "Ridge_Regression-2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 1 point: check the output value\n",
    "assert abs(np.mean(weight_update_function_ridge(a, b, w, 0.001, 0.1)) - 0.99) <= 0.01\n",
    "assert w.shape == weight_update_function_ridge(a, b, w, 0.001, 0.1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "228944becea614c63a4c7c5a6d4551e2",
     "grade": false,
     "grade_id": "cell-32d88540dd3063f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, let's look at a very unbalanced training set. In this example, only 25 instances are used as training set ($17\\%$), and the number of class 0 is significantly higher than both class 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('iris_overfit', 'rb') as f:\n",
    "    x_train, x_val, y_train, y_val = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(np.arange(0, len(y_train), 1), y_train)\n",
    "plt.yticks(np.arange(0, 3, 1))\n",
    "ax.set_xlabel(\"data index\")\n",
    "ax.set_ylabel(\"class label\")\n",
    "ax.set_title(\"Class distribution of training set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09963cd951855227a5e7c42432197740",
     "grade": false,
     "grade_id": "cell-cdee0339af40f024",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For the training, let's do the normal linear regression and ridge regression side by side, with `J_train`, `J_val` recording linear regression training/validation MAE, and `J_train_ridge`, `J_val_ridge` recording ridge regression MAE. Please implement them in the same `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "omega = np.random.randn(5,1)\n",
    "epochs = 3000 # number of updates to the weight\n",
    "alpha = 0.0005 # learning rate\n",
    "lam = 0.5 # coefficient of L2 penalty\n",
    "J_train = np.zeros(epochs) # record MAE of training process (without penalty)\n",
    "J_val = np.zeros(epochs) # record MAE of validation process (without penalty)\n",
    "J_train_ridge = np.zeros(epochs) # record MAE of training process (with penalty)\n",
    "J_val_ridge = np.zeros(epochs) # record MAE of validation process (with penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1eb4b13322781f41730ce945b622ade3",
     "grade": false,
     "grade_id": "Train_Loop_Ridge-answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "omega_norm = np.copy(omega) # weight vector for training without penalty\n",
    "omega_ridge = np.copy(omega) # weight vector for ridge regression\n",
    "\n",
    "for i in range(epochs):\n",
    "    # for each epoch, record the mae for training and validation process without using ridge regression\n",
    "    # use the weight_update_function to update the omega_norm\n",
    "    J_val[i] = None # please update this in your solution\n",
    "    J_train[i] = None # please update this in your solution\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Now, let's try to optimize omega using ridge regression\n",
    "    # use the weight_update_function_ridge to update the omega_ridge\n",
    "    J_val_ridge[i] = None # please update this in your solution\n",
    "    J_train_ridge[i] = None # please update this in your solution\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2efa1a61756be3604cfc5c52cac7c30",
     "grade": true,
     "grade_id": "Train_Loop_Ridge-test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 2 point: check the results of the training process\n",
    "assert J_train_ridge[-1] > 0\n",
    "assert np.std(J_val_ridge[-10:]) <= 1e-3 # the model should converge after training\n",
    "\n",
    "# Hidden test below\n",
    "# 1 point: further tests of the training results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b13bdefa45e339152180be31a55445b",
     "grade": false,
     "grade_id": "cell-edcd9970ea4609f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we plot the training curves again, but this time on a log-log scale to better see the differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curve_ridge(MAE_train, MAE_val, MAE_train_ridge, MAE_val_ridge, epochs):\n",
    "    \"\"\"Plot the mean absolute error for training/validation process\"\"\"\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    ax[0].loglog(np.arange(epochs), MAE_train, label='Training')\n",
    "    ax[0].loglog(np.arange(epochs), MAE_val, label='Validation')\n",
    "    #ax.set_ylim([0,1])\n",
    "    ax[0].set_ylabel(\"Mean Absolute Error\")\n",
    "    ax[0].set_xlabel(\"Epochs\")\n",
    "    ax[0].set_title(\"Without L2 Regularization\")\n",
    "    ax[0].legend(loc='upper right')\n",
    "    \n",
    "    ax[1].loglog(np.arange(epochs), MAE_train_ridge, label='Training')\n",
    "    ax[1].loglog(np.arange(epochs), MAE_val_ridge, label='Validation')\n",
    "    #ax.set_ylim([0,1])\n",
    "    ax[1].set_ylabel(\"Mean Absolute Error\")\n",
    "    ax[1].set_xlabel(\"Epochs\")\n",
    "    ax[1].set_title(\"With L2 Regularization\")\n",
    "    ax[1].legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_curve_ridge(J_train, J_val, J_train_ridge, J_val_ridge, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = prediction(X, omega_norm)\n",
    "Y_pred_ridge = prediction(X, omega_ridge)\n",
    "acc = accuracy(Y_pred, Y)\n",
    "acc_ridge = accuracy(Y_pred_ridge, Y)\n",
    "print(acc)\n",
    "print(acc_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predict_vs_real(Y_pred, Y)\n",
    "plot_predict_vs_real(Y_pred_ridge, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da6badb3722e34bf4e12dc1d1b4f15af",
     "grade": false,
     "grade_id": "cell-11f395039356b11f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Yes or No question:** from the plots/accuracy calculations, do you think the ridge regression improved the learning performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "270be3c130e807ee159eb0bccd4a2266",
     "grade": false,
     "grade_id": "cell-54e049af8010945b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Answer = (\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "064d95d4980d0064674c95967150560a",
     "grade": true,
     "grade_id": "Train_Loop_Ridge-3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5055c03f08fd98b5f02ae7aab78e47f8",
     "grade": false,
     "grade_id": "cell-298117388b266b67",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Scipy Optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06d235aa081fe52b39148d8369007e09",
     "grade": false,
     "grade_id": "cell-1e16607a3299687d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Rather than handcraft the Gradient Decent algorithm, there are libraries that can do the optimization task automatically. The `minimize()` function provided by SciPy library is a good example. It takes as input the name of the function that needs to be minimized, the initial point where the search starts and (optionally) the name of a specific search algorithm. It returns a `OptimizeResult` object that contains details of the optimization result. Below is an example of using `minimize()` function to optimize our linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "np.random.seed(0)\n",
    "omega = np.random.randn(1,5)\n",
    "\n",
    "def mean_square_error(omega):\n",
    "    return (1/(2*len(X)) * np.sum((np.dot(X, omega.reshape(1,5).T) - Y) ** 2 ))\n",
    "\n",
    "\n",
    "res = minimize(mean_square_error, omega, method='L-BFGS-B')\n",
    "print(res)\n",
    "omega_op = np.array(res.x).reshape(5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = prediction(X, omega_op)\n",
    "acc = accuracy(Y_pred, Y)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predict_vs_real(Y_pred, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "35051aafb0110e19e581e1b8603439f6",
     "grade": false,
     "grade_id": "cell-11e0b1c3d98cf009",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can find a good tutorial of SciPy optimization [here](https://machinelearningmastery.com/function-optimization-with-scipy/).  \n",
    "\n",
    "If you have more questions, please refer to the SciPy [documentation](https://docs.scipy.org/doc/scipy/reference/optimize.html).\n",
    "\n",
    "For the introduction of the \"L-BFGS-B\" search algorithm using here, please read [this](http://sepwww.stanford.edu/data/media/public/docs/sep117/antoine1/paper_html/node6.html).\n",
    "\n",
    "***Please do this in a separate notebook that is not submitted!***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6555e4c85d722d3e6de2da681c2fd279",
     "grade": false,
     "grade_id": "cell-4c22e42798de2a83",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Thank you very much for participating the exercise! I hope you find it helpful to understand math basics in machine learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
